{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bpT1IPG9EDnn"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo, RecordEpisodeStatistics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import wandb\n",
    "import os\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "\n",
    "GLOBAL_SEED = 100 # Use 42 for consistency, or any arbitrary fixed number\n",
    "def set_seeds(seed_value):\n",
    "    \"\"\"Sets a fixed seed for reproducibility across all random components.\"\"\"\n",
    "    torch.manual_seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seeds(GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LG9ePLSSEPHr"
   },
   "outputs": [],
   "source": [
    "# Define a Transition named tuple for Experience Replay\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"A fixed-size buffer to store experience tuples.\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition.\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Retrieve a random batch of transitions.\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lVOXCrfzFKUK"
   },
   "outputs": [],
   "source": [
    "class DQN_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Q-Network Model: Takes state as input, outputs Q-values for actions.\n",
    "    This model will be used for both the Online and Target Networks.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN_Model, self).__init__()\n",
    "        # Using a simple 3-layer fully connected network (suitable for CartPole/Acrobot)\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FJ1yqpbmFNzy"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    The main agent class that implements DQN and DDQN logic,\n",
    "    including experience collection and the core training step.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, config):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Hyperparameters (from Wandb config)\n",
    "        self.GAMMA = config.gamma           # Discount Factor\n",
    "        self.LR = config.learning_rate      # NN Learning Rate\n",
    "        self.EPSILON_START = config.epsilon_start\n",
    "        self.EPSILON_END = config.epsilon_end\n",
    "        self.EPSILON_DECAY = config.epsilon_decay # Epsilon Decay Rate\n",
    "        self.BATCH_SIZE = config.batch_size # Learning Batch Size\n",
    "        self.DDQN = config.model == 'DDQN'  # Flag to switch between DQN/DDQN\n",
    "        self.TARGET_UPDATE = config.target_update_freq\n",
    "        self.GLOBAL_SEED = config.seed\n",
    "        self.step_count = 0\n",
    "\n",
    "        # Initialize Networks\n",
    "        self.policy_net = DQN_Model(state_size, action_size).to(self.device)\n",
    "        self.target_net = DQN_Model(state_size, action_size).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval() # Target network is not trained\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.LR)\n",
    "        self.memory = ReplayBuffer(config.memory_size)\n",
    "        self.loss_fn = nn.MSELoss() # Or nn.SmoothL1Loss (Huber Loss)\n",
    "\n",
    "    def select_action(self, state, evaluate=False):\n",
    "        \"\"\"Selects an action using the epsilon-greedy policy.\"\"\"\n",
    "        # Calculate current epsilon based on decay rate\n",
    "        epsilon = self.EPSILON_END + (self.EPSILON_START - self.EPSILON_END) * \\\n",
    "                  np.exp(-self.step_count * self.EPSILON_DECAY)\n",
    "\n",
    "        # In evaluation, always exploit (greedy)\n",
    "        if evaluate or random.random() > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "                # Select action with max Q-value\n",
    "                action = self.policy_net(state_tensor).argmax(1).item()\n",
    "        else:\n",
    "            # Explore: select a random action\n",
    "            action = random.randrange(self.action_size)\n",
    "\n",
    "        return action, epsilon\n",
    "\n",
    "    def update_target_net(self):\n",
    "        \"\"\"Update the target network by copying weights from the policy network.\"\"\"\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    def optimize_model(self):\n",
    "        \"\"\"Performs a single step of optimization on the Policy Network.\"\"\"\n",
    "        if len(self.memory) < self.BATCH_SIZE:\n",
    "            return 0.0 # Not enough samples yet\n",
    "\n",
    "        transitions = self.memory.sample(self.BATCH_SIZE)\n",
    "        # Transpose the batch (Turn an array of Transitions into a Transition of arrays)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Convert state/action/reward arrays to PyTorch tensors\n",
    "        state_batch = torch.stack(list(batch.state)).to(self.device)\n",
    "        action_batch = torch.tensor(batch.action, dtype=torch.int64).unsqueeze(1).to(self.device)\n",
    "        reward_batch = torch.tensor(batch.reward, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "        next_state_batch = torch.stack(list(batch.next_state)).to(self.device)\n",
    "        done_batch = torch.tensor(batch.done, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "\n",
    "        # 1. Compute Q(s_t, a) - the Q-value from the policy network for the action taken\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # 2. Compute V(s_{t+1}) = max_a Q(s_{t+1}, a) - the target value\n",
    "\n",
    "        # CORE DIFFERENCE between DQN and DDQN lies here:\n",
    "        if self.DDQN:\n",
    "            # DDQN: Select action a' using the ONLINE network, then evaluate a' using the TARGET network.\n",
    "            # a'_max = argmax_a Q_online(s', a)\n",
    "            # V(s') = Q_target(s', a'_max)\n",
    "            with torch.no_grad():\n",
    "                # Get the action a' that maximizes Q in the next state, from the POLICY (online) net\n",
    "                next_state_actions = self.policy_net(next_state_batch).argmax(1).unsqueeze(1)\n",
    "\n",
    "            # Compute the Q-value for that selected action a' from the TARGET net\n",
    "            next_state_values = self.target_net(next_state_batch).gather(1, next_state_actions)\n",
    "        else:\n",
    "            # DQN: Select and evaluate the best next action using only the TARGET network.\n",
    "            # V(s') = max_a Q_target(s', a)\n",
    "            with torch.no_grad():\n",
    "                # Compute the max Q-value for the next state from the TARGET net\n",
    "                next_state_values = self.target_net(next_state_batch).max(1)[0].unsqueeze(1)\n",
    "\n",
    "        # Handle terminal states: max Q is 0 if the episode is done\n",
    "        next_state_values = next_state_values * (1 - done_batch)\n",
    "\n",
    "        # Compute the Target Q-Value: Y_t = r_t + gamma * V(s_{t+1})\n",
    "        expected_state_action_values = reward_batch + (self.GAMMA * next_state_values)\n",
    "\n",
    "        # Compute Loss\n",
    "        loss = self.loss_fn(state_action_values, expected_state_action_values)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Clip gradients to prevent large updates\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Target network update\n",
    "        if self.step_count % self.TARGET_UPDATE == 0:\n",
    "            self.update_target_net()\n",
    "\n",
    "        return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Y1N8e9N3FVsv"
   },
   "outputs": [],
   "source": [
    "def train_agent(env, agent, num_episodes, env_name):\n",
    "    \"\"\"The main training loop.\"\"\"\n",
    "    print(f\"\\n--- Starting Training for {agent.config.model} on {env_name} ---\")\n",
    "\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        seed_arg = agent.GLOBAL_SEED if i_episode == 1 else None\n",
    "        state, info = env.reset(seed=seed_arg)\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=agent.device).unsqueeze(0)\n",
    "        episode_reward = 0\n",
    "        loss = 0\n",
    "\n",
    "        for t in itertools.count():\n",
    "            # Select action\n",
    "            action, epsilon = agent.select_action(state.squeeze(0).cpu().numpy())\n",
    "\n",
    "            # Execute action\n",
    "            next_state_np, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Convert to tensors\n",
    "            next_state = torch.tensor(next_state_np, dtype=torch.float32, device=agent.device).unsqueeze(0)\n",
    "            reward = torch.tensor([reward], dtype=torch.float32)\n",
    "\n",
    "            # Store the transition in the Replay Buffer\n",
    "            agent.memory.push(state.squeeze(0), action, reward, next_state.squeeze(0), done)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "            episode_reward += reward.item()\n",
    "            agent.step_count += 1\n",
    "\n",
    "            # Perform one optimization step\n",
    "            current_loss = agent.optimize_model()\n",
    "            loss += current_loss\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Log episode results to Wandb\n",
    "        wandb.log({\n",
    "            \"episode\": i_episode,\n",
    "            \"episode_reward\": episode_reward,\n",
    "            \"avg_step_loss\": loss / t if t > 0 else 0,\n",
    "            \"epsilon\": epsilon,\n",
    "            \"episode_length\": t,\n",
    "        })\n",
    "\n",
    "        if i_episode % 100 == 0:\n",
    "            print(f\"Episode: {i_episode}/{num_episodes} | Reward: {episode_reward:.2f} | Epsilon: {epsilon:.4f}\")\n",
    "\n",
    "    print(f\"--- Training finished for {agent.config.model} ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HKUGGlJDFZiD"
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(env_name, agent, num_tests=100, record_video=False):\n",
    "    \"\"\"Evaluates the trained agent for a number of episodes and logs duration.\"\"\"\n",
    "    print(f\"\\n--- Starting Evaluation for {agent.config.model} on {env_name} ({num_tests} tests) ---\")\n",
    "\n",
    "    # Determine render mode\n",
    "    render_mode = \"rgb_array\" if record_video else None\n",
    "\n",
    "    # Create evaluation environment (must use render_mode='rgb_array' for video)\n",
    "    eval_env = gym.make(env_name, render_mode=render_mode)\n",
    "    # Wrap for collecting episode statistics\n",
    "    # The 'deque_size' argument is deprecated/removed in newer Gymnasium versions.\n",
    "    eval_env = RecordEpisodeStatistics(eval_env)\n",
    "\n",
    "    # Wrap for video recording (if requested)\n",
    "    if record_video:\n",
    "        video_folder = f\"./videos/{env_name}_{agent.config.model}\"\n",
    "        # Only record the first test episode\n",
    "        eval_env = RecordVideo(\n",
    "            eval_env,\n",
    "            video_folder=video_folder,\n",
    "            episode_trigger=lambda x: x == 0,\n",
    "            name_prefix=f\"best_agent\"\n",
    "        )\n",
    "        print(f\"Recording the first episode to: {video_folder}\")\n",
    "\n",
    "    # Run tests\n",
    "    test_durations = []\n",
    "    test_rewards = []\n",
    "\n",
    "    for i in range(num_tests):\n",
    "        state, info = eval_env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action, _ = agent.select_action(state, evaluate=True)\n",
    "            state, reward, terminated, truncated, info = eval_env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # RecordEpisodeStatistics wrapper adds episode stats to info on done=True\n",
    "            if done:\n",
    "                if 'episode' in info:\n",
    "                    duration = info['episode']['l']\n",
    "                    reward = info['episode']['r']\n",
    "\n",
    "                    # Ensure duration and reward are floats\n",
    "                    if isinstance(duration, int):\n",
    "                        duration = float(duration)\n",
    "                    if isinstance(reward, int):\n",
    "                        reward = float(reward)\n",
    "\n",
    "                    test_durations.append(duration)\n",
    "                    test_rewards.append(reward)\n",
    "\n",
    "                    # Log individual test result\n",
    "                    wandb.log({\n",
    "                        f\"{env_name}/Test_Episode_Duration\": duration,\n",
    "                        f\"{env_name}/Test_Episode_Reward\": reward,\n",
    "                        \"test_episode_index\": i\n",
    "                    })\n",
    "                break\n",
    "\n",
    "    eval_env.close()\n",
    "\n",
    "    if test_durations:\n",
    "        avg_duration = np.mean(test_durations)\n",
    "        std_duration = np.std(test_durations)\n",
    "        avg_reward = np.mean(test_rewards)\n",
    "\n",
    "        wandb.log({\n",
    "            f\"{env_name}/Avg_Test_Duration\": avg_duration,\n",
    "            f\"{env_name}/Std_Test_Duration\": std_duration,\n",
    "            f\"{env_name}/Avg_Test_Reward\": avg_reward,\n",
    "        })\n",
    "\n",
    "        print(f\"Evaluation complete. Avg Duration: {avg_duration:.2f} ± {std_duration:.2f} steps.\")\n",
    "        print(f\"Avg Reward: {avg_reward:.2f}\")\n",
    "\n",
    "    return avg_reward, avg_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-Qq5kElZFdzx"
   },
   "outputs": [],
   "source": [
    "def main_run(config):\n",
    "    \"\"\"Initializes Wandb, environment, agent, trains, and evaluates.\"\"\"\n",
    "    # 1. Initialize Wandb Run\n",
    "    run = wandb.init(\n",
    "        project=\"CartPole-v1-problem-seed-right-value-v8\",\n",
    "        name=f\"{config['model']}_DF{config['gamma']}_EDR{config['epsilon_decay']}_NNLR{config['learning_rate']}_MEM{config['memory_size']}_BS{config['batch_size']}\",\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    # 2. Setup Environment\n",
    "    # Note: Pendulum-v1 and MountainCar-v0 have Continuous Action Spaces.\n",
    "    # We discretize them for Q-Learning compatibility.\n",
    "\n",
    "    env_name = config['env_name']\n",
    "\n",
    "    if env_name in [\"Pendulum-v1\", \"MountainCar-v0\"]:\n",
    "        # Discretize continuous environments for Q-Learning\n",
    "        if env_name == \"Pendulum-v1\":\n",
    "             # Actions: 5 discrete actions: max_torque * [-2.0, -1.0, 0.0, 1.0, 2.0]\n",
    "             env = gym.make(env_name, max_episode_steps=200) # Default\n",
    "             class ContinuousActionWrapper(gym.ActionWrapper):\n",
    "                 def __init__(self, env):\n",
    "                     super().__init__(env)\n",
    "                     self.action_range = [-2.0, -1.0, 0.0, 1.0, 2.0]\n",
    "                     self.action_space = gym.spaces.Discrete(len(self.action_range))\n",
    "                 def action(self, action_idx):\n",
    "                     # Map the discrete index to the continuous action value\n",
    "                     return np.array([self.action_range[action_idx]], dtype=np.float32)\n",
    "             env = ContinuousActionWrapper(env)\n",
    "\n",
    "        elif env_name == \"MountainCar-v0\":\n",
    "            # Actions: 3 discrete actions: 0:push_left, 1:no_push, 2:push_right\n",
    "            env = gym.make(env_name, max_episode_steps=200) # Default\n",
    "\n",
    "    elif env_name in [\"CartPole-v1\", \"Acrobot-v1\"]:\n",
    "        env = gym.make(env_name)\n",
    "    else:\n",
    "        raise ValueError(f\"Environment {env_name} not supported by this script.\")\n",
    "\n",
    "    # Get environment specs\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    # 3. Create Agent\n",
    "    agent = DQNAgent(state_size, action_size, wandb.config)\n",
    "\n",
    "    # 4. Train\n",
    "    #Set episodes based on difficulty (you can adjust these)\n",
    "    if env_name == \"CartPole-v1\":\n",
    "        num_episodes = 500\n",
    "    elif env_name == \"Acrobot-v1\":\n",
    "        num_episodes = 1000\n",
    "    elif env_name == \"MountainCar-v0\":\n",
    "        num_episodes = 2000\n",
    "    elif env_name == \"Pendulum-v1\":\n",
    "        num_episodes = 1000\n",
    "\n",
    "    train_agent(env, agent, 200, env_name)\n",
    "    env.close()\n",
    "\n",
    "    # 5. Evaluate (100 tests)\n",
    "    # evaluate_agent(env_name, agent, num_tests=100, record_video=False)\n",
    "\n",
    "    # 6. Record the Best Agent (Separate run for the final deliverable)\n",
    "    # The record_video=True will only record the FIRST episode of the 100 tests.\n",
    "    # Run this function separately after identifying your best hyperparameter setup.\n",
    "    evaluate_agent(env_name, agent, num_tests=100, record_video=True)\n",
    "\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GHTCtIKQFlQK"
   },
   "outputs": [],
   "source": [
    "BASELINE_CONFIG = {\n",
    "    \"env_name\": \"CartPole-v1\",       # Environment to run (change this for other envs)\n",
    "    \"model\": \"DQN\",                 # Can be 'DQN' or 'DDQN'\n",
    "    \"num_episodes\": 500,            # Training episodes (overwritten in main_run)\n",
    "    \"gamma\": 0.99,                  # Discount Factor\n",
    "    \"learning_rate\": 2e-4,          # NN Learning Rate\n",
    "    \"epsilon_start\": 1.0,           # Start exploration rate\n",
    "    \"epsilon_end\": 0.01,            # Minimum exploration rate\n",
    "    \"epsilon_decay\": 0.001,         # Epsilon Decay Rate (adjust to control exploration speed)\n",
    "    \"memory_size\": 50000,           # Replay Memory Size\n",
    "    \"batch_size\": 64,               # Learning Batch Size\n",
    "    \"target_update_freq\": 200,      # Target network update frequency (in steps)\n",
    "    \"seed\": 100,\n",
    "}\n",
    "\n",
    "MOUNTAINCAR_BASELINE_CONFIG = {\n",
    "        \"env_name\": \"MountainCar-v0\",\n",
    "        \"model\": \"DQN\",\n",
    "        \"num_episodes\": 2000,          \n",
    "        \"gamma\": 0.999,                \n",
    "        \"learning_rate\": 2e-4,         \n",
    "        \"epsilon_start\": 1.0,\n",
    "        \"epsilon_end\": 0.01,\n",
    "        \"epsilon_decay\": 0.0005,        \n",
    "        \"memory_size\": 50000,\n",
    "        \"batch_size\": 32,\n",
    "        \"target_update_freq\": 200,\n",
    "        \"seed\": 100,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xPpc0XrQFt2r",
    "outputId": "3caf7e9b-5b22-4795-e85f-14287f1e5f6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Baseline Run: DQN on CartPole-v1. Check your Wandb dashboard.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: amira-elgarf02 (amira-elgarf02-cairo-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>F:\\Uni\\fall2026\\RL\\Ass\\ass2\\wandb\\run-20251112_220654-rpbyj43o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amira-elgarf02-cairo-university/CartPole-v1-problem-seed-right-value-v8/runs/rpbyj43o' target=\"_blank\">DQN_DF0.99_EDR0.001_NNLR0.0002_MEM50000_BS64</a></strong> to <a href='https://wandb.ai/amira-elgarf02-cairo-university/CartPole-v1-problem-seed-right-value-v8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amira-elgarf02-cairo-university/CartPole-v1-problem-seed-right-value-v8' target=\"_blank\">https://wandb.ai/amira-elgarf02-cairo-university/CartPole-v1-problem-seed-right-value-v8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amira-elgarf02-cairo-university/CartPole-v1-problem-seed-right-value-v8/runs/rpbyj43o' target=\"_blank\">https://wandb.ai/amira-elgarf02-cairo-university/CartPole-v1-problem-seed-right-value-v8/runs/rpbyj43o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training for DQN on CartPole-v1 ---\n",
      "Episode: 100/200 | Reward: 317.00 | Epsilon: 0.0100\n",
      "Episode: 200/200 | Reward: 358.00 | Epsilon: 0.0100\n",
      "--- Training finished for DQN ---\n",
      "\n",
      "--- Starting Evaluation for DQN on CartPole-v1 (100 tests) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amira Elgarf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: WARN: Overwriting existing videos at F:\\Uni\\fall2026\\RL\\Ass\\ass2\\videos\\CartPole-v1_DQN folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording the first episode to: ./videos/CartPole-v1_DQN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amira Elgarf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Avg Duration: 352.59 ± 75.48 steps.\n",
      "Avg Reward: 352.59\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>CartPole-v1/Avg_Test_Duration</td><td>▁</td></tr><tr><td>CartPole-v1/Avg_Test_Reward</td><td>▁</td></tr><tr><td>CartPole-v1/Std_Test_Duration</td><td>▁</td></tr><tr><td>CartPole-v1/Test_Episode_Duration</td><td>▃▇▂█▂▂▇▃▄▂█▂▄▄▄▅▄▁▁▅▃▃▃█▂▅▄▃▆▂█▃▄▆▁▃▄▇▂▃</td></tr><tr><td>CartPole-v1/Test_Episode_Reward</td><td>▂▃▅▇▁▆▂▇▃▃▅█▅▄▄▄▄▅▅▄▃▂▃█▄█▄▃▆▂▆▂▃▃▁▃█▇█▃</td></tr><tr><td>avg_step_loss</td><td>▂▂▁▁▁▂▁▁▁▁▂▁▂▂▂▂▂▃▃▃▃▄▄▅▅▆▅▅▆▅▇█▆▇▆▆▆▅▅▇</td></tr><tr><td>episode</td><td>▁▁▂▂▂▂▂▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇██████</td></tr><tr><td>episode_length</td><td>▂▁▁▁▁▁▁▁▁▁▁▁▂▅▇▇▆▅▇▇▅▆▅▅▇▆▆▅▅▆▆▇▆▇▆▅▇▆▆█</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▅▆▄▅▅▄▇▆▆▆▆▄▄▄▅▅▅▄▄▆▄▅▅█▆</td></tr><tr><td>epsilon</td><td>█▇▇▆▆▅▅▅▄▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>+1</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>CartPole-v1/Avg_Test_Duration</td><td>352.59</td></tr><tr><td>CartPole-v1/Avg_Test_Reward</td><td>352.59</td></tr><tr><td>CartPole-v1/Std_Test_Duration</td><td>75.48021</td></tr><tr><td>CartPole-v1/Test_Episode_Duration</td><td>315</td></tr><tr><td>CartPole-v1/Test_Episode_Reward</td><td>315</td></tr><tr><td>avg_step_loss</td><td>3.83539</td></tr><tr><td>episode</td><td>200</td></tr><tr><td>episode_length</td><td>357</td></tr><tr><td>episode_reward</td><td>358</td></tr><tr><td>epsilon</td><td>0.01</td></tr><tr><td>+1</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DQN_DF0.99_EDR0.001_NNLR0.0002_MEM50000_BS64</strong> at: <a href='https://wandb.ai/amira-elgarf02-cairo-university/CartPole-v1-problem-seed-right-value-v8/runs/rpbyj43o' target=\"_blank\">https://wandb.ai/amira-elgarf02-cairo-university/CartPole-v1-problem-seed-right-value-v8/runs/rpbyj43o</a><br> View project at: <a href='https://wandb.ai/amira-elgarf02-cairo-university/CartPole-v1-problem-seed-right-value-v8' target=\"_blank\">https://wandb.ai/amira-elgarf02-cairo-university/CartPole-v1-problem-seed-right-value-v8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251112_220654-rpbyj43o\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Ensure videos folder exists\n",
    "    os.makedirs(\"./videos\", exist_ok=True)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 1: INITIAL RUN (DQN, CartPole-v1)\n",
    "    # This will establish a baseline and verify your code works.\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"Executing Baseline Run: DQN on CartPole-v1. Check your Wandb dashboard.\")\n",
    "    config_baseline = deepcopy(BASELINE_CONFIG)\n",
    "    config_baseline['env_name'] = 'CartPole-v1'\n",
    "    config_baseline['model'] = 'DQN'\n",
    "    main_run(config_baseline)\n",
    "\n",
    "    # print(\"Executing Baseline Run: DQN on CartPole-v1. Check your Wandb dashboard.\")\n",
    "    # config_baseline = deepcopy(BASELINE_CONFIG)\n",
    "    # config_baseline['env_name'] = 'CartPole-v1'\n",
    "    # config_baseline['model'] = 'DDQN'\n",
    "    # main_run(config_baseline)\n",
    "    #----------------------------------cartpole-----------------\n",
    "    # # R1: DQN Baseline (Control Group)\n",
    "    # config_r1 = deepcopy(BASELINE_CONFIG)\n",
    "    # config_r1['name'] = 'R1_DQN_BASELINE'\n",
    "\n",
    "    # # R2: DDQN Baseline (Model Comparison)\n",
    "    # config_r2 = deepcopy(BASELINE_CONFIG)\n",
    "    # config_r2['model'] = 'DDQN'\n",
    "    # config_r2['name'] = 'R2_DDQN_COMPARISON'\n",
    "    \n",
    "    # # --- DISCOUNT FACTOR (GAMMA) ---\n",
    "    # # R3: Gamma Too Low (Myopic Agent)\n",
    "    # config_r3 = deepcopy(BASELINE_CONFIG)\n",
    "    # config_r3['gamma'] = 0.80\n",
    "    # config_r3['name'] = 'R3_GAMMA_LOW_0.80'\n",
    "    \n",
    "    # # R4: Gamma Too High (Farsighted/Unstable Agent)\n",
    "    # config_r4 = deepcopy(BASELINE_CONFIG)\n",
    "    # config_r4['gamma'] = 0.999\n",
    "    # config_r4['name'] = 'R4_GAMMA_HIGH_0.999'\n",
    "\n",
    "    # # --- NN LEARNING RATE (LR) ---\n",
    "    # # R5: LR Too High (Divergence/Oscillation)\n",
    "    # config_r5 = deepcopy(BASELINE_CONFIG)\n",
    "    # config_r5['learning_rate'] = 0.01  # 20x higher than baseline\n",
    "    # config_r5['name'] = 'R5_LR_HIGH_0.01'\n",
    "\n",
    "    # # R6: LR Too Low (Slow Convergence)\n",
    "    # config_r6 = deepcopy(BASELINE_CONFIG)\n",
    "    # config_r6['learning_rate'] = 1e-5\n",
    "    # config_r6['name'] = 'R6_LR_LOW_1e-5'\n",
    "\n",
    "    # # --- EPSILON DECAY RATE (ALPHA) ---\n",
    "    # # R7: Decay Too Slow (Persistent Exploration) - 10x slower\n",
    "    # config_r7 = deepcopy(BASELINE_CONFIG)\n",
    "    # config_r7['epsilon_decay'] = 0.0005\n",
    "    # config_r7['name'] = 'R7_DECAY_SLOW_0.0005'\n",
    "    \n",
    "    # # R8: Decay Too Fast (Premature Exploitation) - 10x faster\n",
    "    # config_r8 = deepcopy(BASELINE_CONFIG)\n",
    "    # config_r8['epsilon_decay'] = 0.05\n",
    "    # config_r8['name'] = 'R8_DECAY_FAST_0.05'\n",
    "\n",
    "    # # --- MEMORY SIZE & BATCH SIZE ---\n",
    "    # # R9: Small Replay Memory (High Correlation)\n",
    "    # config_r9 = deepcopy(BASELINE_CONFIG)\n",
    "    # config_r9['memory_size'] = 5000\n",
    "    # config_r9['name'] = 'R9_MEM_SMALL_5k'\n",
    "\n",
    "    # # R10: Small Batch Size (Noisy Gradients)\n",
    "    # config_r10 = deepcopy(BASELINE_CONFIG)\n",
    "    # config_r10['batch_size'] = 16\n",
    "    # config_r10['name'] = 'R10_BATCH_SMALL_16'\n",
    "\n",
    "\n",
    "    # # --- EXECUTION LOOP ---\n",
    "    # experiment_configs = [\n",
    "    #     config_r1, config_r2, config_r3, config_r4, config_r5, \n",
    "    #     config_r6, config_r7, config_r8, config_r9, config_r10\n",
    "    # ]\n",
    "\n",
    "    # for i, config in enumerate(experiment_configs):\n",
    "    #     print(f\"\\n========================================================\")\n",
    "    #     print(f\"Starting Experiment {i+1}/{len(experiment_configs)}: {config['name']}\")\n",
    "    #     print(f\"========================================================\")\n",
    "        \n",
    "    #     # Log the specific config name to Wandb for easy identification\n",
    "    #     config_to_run = deepcopy(config)\n",
    "        \n",
    "    #     # NOTE: If you are running this in a notebook, you may need to restart\n",
    "    #     # the kernel between runs to ensure Wandb is initialized correctly.\n",
    "        \n",
    "    #     # Run the experiment\n",
    "    #     main_run(config_to_run)\n",
    "        \n",
    "    # print(\"\\n\\nALL 10 CARTPOLE EXPERIMENTS COMPLETE. CHECK WANDB FOR RESULTS.\")\n",
    "    \n",
    "    # # R1: DQN Baseline (Control Group - Optimized for Sparse Reward)\n",
    "    # config_r1 = deepcopy(MOUNTAINCAR_BASELINE_CONFIG)\n",
    "    # config_r1['name'] = 'MC_R1_DQN_BASELINE_OPTM'\n",
    "\n",
    "    # # R2: DDQN Baseline (Model Comparison)\n",
    "    # config_r2 = deepcopy(MOUNTAINCAR_BASELINE_CONFIG)\n",
    "    # config_r2['model'] = 'DDQN'\n",
    "    # config_r2['name'] = 'MC_R2_DDQN_COMPARISON'\n",
    "    \n",
    "    # # --- DISCOUNT FACTOR (GAMMA) ---\n",
    "    # # R3: Gamma Too Low (Myopic Agent - Expected to Fail)\n",
    "    # config_r3 = deepcopy(MOUNTAINCAR_BASELINE_CONFIG)\n",
    "    # config_r3['gamma'] = 0.95  # Significantly lower than 0.999\n",
    "    # config_r3['name'] = 'MC_R3_GAMMA_LOW_0.95'\n",
    "    \n",
    "    # # R4: Gamma Very High (Testing Edge Case)\n",
    "    # config_r4 = deepcopy(MOUNTAINCAR_BASELINE_CONFIG)\n",
    "    # config_r4['gamma'] = 1.0 # Perfect Discount\n",
    "    # config_r4['name'] = 'MC_R4_GAMMA_PERFECT_1.0'\n",
    "\n",
    "    # # --- NN LEARNING RATE (LR) ---\n",
    "    # # R5: LR Too High (Divergence/Oscillation)\n",
    "    # config_r5 = deepcopy(MOUNTAINCAR_BASELINE_CONFIG)\n",
    "    # config_r5['learning_rate'] = 0.01\n",
    "    # config_r5['name'] = 'MC_R5_LR_HIGH_0.01'\n",
    "\n",
    "    # # R6: LR Too Low (Slow Convergence)\n",
    "    # config_r6 = deepcopy(MOUNTAINCAR_BASELINE_CONFIG)\n",
    "    # config_r6['learning_rate'] = 1e-5\n",
    "    # config_r6['name'] = 'MC_R6_LR_LOW_1e-5'\n",
    "\n",
    "    # # --- EPSILON DECAY RATE (ALPHA) ---\n",
    "    # # R7: Decay Too Slow (Persistent Exploration - Baseline is already slow, make it ultra-slow)\n",
    "    # config_r7 = deepcopy(MOUNTAINCAR_BASELINE_CONFIG)\n",
    "    # config_r7['epsilon_decay'] = 0.0001\n",
    "    # config_r7['name'] = 'MC_R7_DECAY_ULTRA_SLOW_0.0001'\n",
    "    \n",
    "    # # R8: Decay Too Fast (Premature Exploitation - Expected to Fail)\n",
    "    # config_r8 = deepcopy(MOUNTAINCAR_BASELINE_CONFIG)\n",
    "    # config_r8['epsilon_decay'] = 0.01 # 10x faster than baseline\n",
    "    # config_r8['name'] = 'MC_R8_DECAY_FAST_0.01'\n",
    "\n",
    "    # # --- MEMORY SIZE & BATCH SIZE ---\n",
    "    # # R9: Small Replay Memory (High Correlation)\n",
    "    # config_r9 = deepcopy(MOUNTAINCAR_BASELINE_CONFIG)\n",
    "    # config_r9['memory_size'] = 5000\n",
    "    # config_r9['name'] = 'MC_R9_MEM_SMALL_5k'\n",
    "\n",
    "    # # R10: Large Batch Size (Smoother Gradients, but may hinder exploration on sparse rewards)\n",
    "    # config_r10 = deepcopy(MOUNTAINCAR_BASELINE_CONFIG)\n",
    "    # config_r10['batch_size'] = 128\n",
    "    # config_r10['name'] = 'MC_R10_BATCH_LARGE_128'\n",
    "\n",
    "\n",
    "    # # --- EXECUTION LOOP ---\n",
    "    # experiment_configs = [\n",
    "    #     config_r1, config_r2, config_r3, config_r4, config_r5, \n",
    "    #     config_r6, config_r7, config_r8, config_r9, config_r10\n",
    "    # ]\n",
    "\n",
    "    # for i, config in enumerate(experiment_configs):\n",
    "    #     print(f\"\\n========================================================\")\n",
    "    #     print(f\"Starting Experiment {i+1}/{len(experiment_configs)}: {config['name']}\")\n",
    "    #     print(f\"========================================================\")\n",
    "        \n",
    "    #     # Log the specific config name to Wandb for easy identification\n",
    "    #     config_to_run = deepcopy(config)\n",
    "        \n",
    "    #     # NOTE: If you are running this in a notebook, you may need to restart\n",
    "    #     # the kernel between runs to ensure Wandb is initialized correctly.\n",
    "        \n",
    "    #     # Run the experiment\n",
    "    #     main_run(config_to_run)\n",
    "        \n",
    "    # print(\"\\n\\nALL 10 MOUNTAINCAR EXPERIMENTS COMPLETE. CHECK WANDB FOR RESULTS.\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 2: HYPERPARAMETER SEARCH AND DDQN COMPARISON (Step 8)\n",
    "    # Uncomment and modify these blocks to run your full experiment matrix.\n",
    "    # Use different parameter values for each run to test their effect!\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # # Example DDQN run for comparison\n",
    "    # config_ddqn = deepcopy(BASELINE_CONFIG)\n",
    "    # config_ddqn['model'] = 'DDQN'\n",
    "    # # main_run(config_ddqn)\n",
    "\n",
    "    # # Example DQN run with high Learning Rate (to test LR effect)\n",
    "    # config_lr_high = deepcopy(BASELINE_CONFIG)\n",
    "    # config_lr_high['learning_rate'] = 0.01\n",
    "    # # main_run(config_lr_high)\n",
    "\n",
    "    # # Example DDQN run on Acrobot-v1\n",
    "    # config_acrobot = deepcopy(BASELINE_CONFIG)\n",
    "    # config_acrobot['env_name'] = 'Acrobot-v1'\n",
    "    # config_acrobot['model'] = 'DDQN'\n",
    "    # config_acrobot['g\n",
    "    # \n",
    "    # \n",
    "    # \n",
    "    # 995 # Acrobot needs a higher gamma\n",
    "    # # main_run(config_acrobot)\n",
    "\n",
    "    # # Example DDQN run on MountainCar-v0\n",
    "    # config_mountaincar = deepcopy(BASELINE_CONFIG)\n",
    "    # config_mountaincar['env_name'] = 'MountainCar-v0'\n",
    "    # config_mountaincar['model'] = 'DDQN'\n",
    "    # config_mountaincar['gamma'] = 1.0 # Requires high/perfect discount due to sparse reward\n",
    "    # config_mountaincar['epsilon_decay'] = 0.001 # Slower exploration decay\n",
    "    # # main_run(config_mountaincar)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 3: FINAL VIDEO RECORDING (Step 9)\n",
    "    # Once you find your best performing model and hyperparameters, run the\n",
    "    # evaluation *only* to generate the video, then stop this script.\n",
    "    # Replace the configuration below with your best setup.\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # IMPORTANT: The video generation function is not run by default.\n",
    "    # To record your final video, manually run the following after finding\n",
    "    # your best agent/config:\n",
    "\n",
    "    # BEST_CONFIG = deepcopy(config_acrobot) # Example: use your best Acrobot config\n",
    "    # FINAL_ENV = 'Acrobot-v1'\n",
    "    #\n",
    "    # final_env = gym.make(FINAL_ENV)\n",
    "    # if FINAL_ENV == 'Pendulum-v1': # Re-apply wrapper if needed\n",
    "    #      class ContinuousActionWrapper(gym.ActionWrapper):\n",
    "    #          def __init__(self, env):\n",
    "    #              super().__init__(env)\n",
    "    #              self.action_range = [-2.0, -1.0, 0.0, 1.0, 2.0]\n",
    "    #              self.action_space = gym.spaces.Discrete(len(self.action_range))\n",
    "    #          def action(self, action_idx):\n",
    "    #              return np.array([self.action_range[action_idx]], dtype=np.float32)\n",
    "    #      final_env = ContinuousActionWrapper(final_env)\n",
    "    #\n",
    "    # final_agent = DQNAgent(final_env.observation_space.shape[0], final_env.action_space.n, BEST_CONFIG)\n",
    "    # # NOTE: You would typically load the trained weights here,\n",
    "    # # but for this self-contained script, you might re-run the training\n",
    "    # # and then immediately call the evaluation with record_video=True\n",
    "    #\n",
    "    # # Temporarily re-run the best training once more\n",
    "    # print(f\"Recording final video for {FINAL_ENV} with best DDQN agent...\")\n",
    "    # # train_agent(final_env, final_agent, 1000, FINAL_ENV) # If you need to re-train\n",
    "    # # evaluate_agent(FINAL_ENV, final_agent, num_tests=1, record_video=True)\n",
    "    # final_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
